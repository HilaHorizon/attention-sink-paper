\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (camera-ready) version.
\usepackage[review]{acl}

% Standard package includes recommended by the ACL template
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{mathtools}
% Additional packages you were using
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
% Make figure paths robust regardless of working directory
\graphicspath{{./}{./figures/}{./figures/no_intervention/}{./figures/intervention1/}{./figures/intervention2/}{./figures/intervention3/}{./figures/intervention4/}{./figures/intervention5/}{./figures/intervention5_2/}{./figures/obs1_appendix/}{./figures/obs2_appendix/}}
\usepackage{cleveref}
\usepackage{enumitem} % for list control
% Add these packages to the preamble (near your other \usepackage lines)
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{caption}
\usepackage{subcaption} % optional, if you want subfigures later
% If title/author block needs more space, uncomment and change:
% \setlength\titlebox{6cm}
\usepackage{float}
\usepackage{xcolor}
% \YRMcommentstrue makes \YRM show comments, \YRMcommentsfalse makes it do nothing.
\newif\ifYRMcomments
\newif\ifBacklogcomments
\newif\ifResolvedcomments

\Backlogcommentsfalse


%\YRMcommentstrue % set to \YRMcommentsfalse to hide comments
%\Resolvedcommentstrue
% \Backlogcommentstrue

\YRMcommentsfalse
\Resolvedcommentsfalse

\newcommand{\YTODO}[1]{\ifYRMcomments\textcolor{pink}{[TODO for Yuval: #1]}\fi}

\newcommand{\YRM}[1]{\ifYRMcomments\textcolor{red}{[YRM: #1]}\fi}

\newcommand{\Backlog}[1]{\ifBacklogcomments\textcolor{blue}{[Backlog: #1]}\fi}

\newcommand{\Resolved}[1]{\ifResolvedcomments\textcolor{green}{[Resolved: #1]}\fi}

\title{No Universal Mechanism for Attention Sink in Transformers: \\Evidence from GPT-2}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  \texttt{email@domain} \\
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
  Transformers commonly exhibit an attention sink: disproportionately high attention to the first position. We study this behavior in GPT-2–style models with learned query biases and absolute positional embeddings. Combining analysis with targeted interventions, we find that the sink arises from the interaction among (i) a learned query bias, (ii) the first-layer transformation of the positional encoding and (iii) structure in the key projection. Together with observations of sinks in models without query biases or absolute positional embeddings (e.g., RoPE or ALiBi), this indicates that attention sinks do not arise from a single universal mechanism but instead depend on architecture. These findings inform mitigation of attention sink, and motivate broader investigation of sink mechanisms across different architectures.
\end{abstract}

\section{Introduction}
\YTODO{Write an intro.}

\section{Related Work}
\YRM{I didn't look at this part yet, waiting for you to finish it fist :) } 

\YRM{As mentioned, this needs to be improved - By citing more, making this more concise (as concise as possible while including really vital info, and adding more in appendix if needed, like we did in our attention knockout paper )}Our investigation into the attention sink's origins builds upon four key areas of Transformer research: the methods for encoding positional information, the phenomenon of attention sink, and the recent discovery of massive activations functioning as implicit biases.

\subsection{Positional Encoding in Transformers}
By design, the self-attention mechanism has no inherent sense of token order. To address this, Transformers must be augmented with positional information. The original Transformer used fixed sinusoidal embeddings \citep{vaswani2017attention}. Many models in the GPT family, including the GPT-2 model we investigate, use learned absolute positional embeddings - a vector for each position that is added to the token embedding at input. More recent architectures have introduced alternative methods, such as the LLaMA architecture \citep{touvron2023llama2} which utilizes Rotary Positional Embeddings (RoPE) \citep{su2021roformer}, and Attention with Linear Biases (ALiBi) \citep{press2021train}, which is a key feature in models like BLOOM \citep{bigscience2023bloom}.

\subsection{The Attention Sink Phenomenon}
Recent empirical work has identified a curious and robust phenomenon in auto-regressive language models termed the ``attention sink'' \citep{xiao2023efficient}. This refers to the tendency of models to allocate a significant portion of their attention to token(s) even when they are not semantically important, typically the first one(s) in the sequence. As \citet{gu2025when} demonstrate, this phenomenon is not an anomaly but emerges consistently during pre-training \YRM{This is possibly the most important part of the RW. This should be with much more details and more citations, so saying specially that he demonstrated this in model without query bias and absolute positional encoding}. Some studies have found the attention sink could have a negative impact on the achievable accuracy of LLMs \cite{Yu2024Unveiling} \YRM{Can you add a bit of details here? Citing a lot of papers that write about negative impact, even if they only say so in a sentence, would be great}. A similar phenomenon has been observed in LMMs (Large Multimodal Models) (\citet{Kang2025See}, \cite{wang2025mirage}) and ViTs (Visual transformers) \cite{Feng2025EDIT:}. \citet {Barbero2025Why} argue that attention sinks are a
way for LLMs to avoid over-mixing and representational collapse. The main mechanism for attention sink that has been identified so far is the softmax normalization (\citet{xiao2023efficient}, \citet{gu2025when}, \citet{Zuhri2025SoftpickNA}). Some models, such as Mamba-based models as \citet{endy-etal-2025-mamba} shows, don't exhibit attention sink.

\subsection{Massive Activations}
The phenomenon of \emph{massive activations}—where a tiny subset of coordinates exhibit orders-of-magnitude larger activations—has been identified and characterized across a variety of LLMs (\citet{sun2024massive}, \citet{lindnielsen2024spectral}, \citet{lin2024duquant}) and LMMs \cite{Kang2025See}. Crucially, these massive activations often appear at specific token positions, most notably the very first token of a sequence. \citep{sun2024massive} hypothesize that these activations function as bias terms that are learned by the model. 



\section{Preliminaries}
\YTODO{Write about the input (the sentence we are using)}
\subsection{Attention mechanism}
Let $X^{(i)}=[x_1^{(i)},\ldots,x_n^{(i)}]$ denote the input to attention layer $i$, where $x_t^{(i)}\in\mathbb{R}^{d}$ is the representation for position $t$ (after LayerNorm). We denote projection matrices and biases by $W_q^{(i)},W_k^{(i)},W_v^{(i)}\in\mathbb{R}^{d\times d}$ and $b_Q^{(i)},b_K^{(i)},b_V^{(i)}\in\mathbb{R}^{d}$. Queries, keys, and values are: $q_t^{(i)}=W_q^{(i)}x_t^{(i)} + b_Q^{(i)}$, $k_t^{(i)}=W_k^{(i)}x_t^{(i)} + b_K^{(i)}$, and $v_t^{(i)}=W_v^{(i)}x_t^{(i)} + b_V^{(i)}$. Some architectures include biases \cite{vaswani2017attention}, others omit them \cite{touvron2023llama2}. \YRM{Verify and add more specific citations for bias usage patterns}

For autoregressive generation, attention weights are $\alpha_{t j}=\mathrm{softmax}_j\!\big((q_t^{(i)})^\top k_j^{(i)} / \sqrt{d}\big)$ \YRM{I changed the transpose here to be on the Q, please make sure that OK (this is better for later). Make sure this changes fit all other places/notations)}where the softmax is over valid positions $j \le t$. Multi-head attention divides the feature dimension across $h$ heads, 
computing attention independently within each head's subspace before 
concatenating outputs. For simplicity, our experiments treat $W_k$ and $b_Q$ in their original form prior to head-wise reshaping.

\subsection{Positional encoding}
Attention layers are invariant to input permutations, lacking inherent awareness of token order. To address this limitation, Transformers incorporate positional information through various schemes \YRM{Cite all in RW}. We focus on learned absolute positional encodings: a set of trainable vectors $\{p_i\}_{i=1}^{L} \subset \mathbb{R}^{d}$, where $i$ is the token position, and $L$ is the sequence length. These are added to token embeddings $e_i$ creating the input: $x_i^{(0)} = e_i + p_i$.

\subsubsection{Effective positional encoding (EPE)}
We define the \emph{effective positional encoding} (EPE) for position $i$ as $\mathrm{EPE}_i = \mathrm{MLP}^{(1)}(p_i) + p_i$, where $\mathrm{MLP}^{(1)}$ denotes the first layer's feed-forward network applied to the raw positional encoding $p_i$, and the residual connection preserves the original positional signal.  We term this ``effective'' because it captures the net positional signal that emerges after the first layer's transformation. Specifically, we observe experimentally that adding $\mathrm{EPE}_i$ to the output of the first layer (when no positional encoding was initially provided) produces approximately the same effect as the standard approach of adding the raw positional encoding $p_i$ before the first layer. This equivalence demonstrates that $\mathrm{EPE}_i$ represents the effective contribution of positional information after being processed through the network's initial transformations (Experiments demonstrating this can be found in \label{app:epe_exp}).

\section{Methodology and Results}
First, we state the result of our analysis - a description of the mechanism underlying the attention sink in models with learnable query biases and absolute positional encodings. Then, through experimental analyses and causal interventions we provide evidence for our hypothesis.

\subsection{Result: Mechanism behind the attention sink}\label{sec:mechanism}
Consider layer $i$. Before softmax (and scaling), the attention score from source position $t$ to target position $j$ is $s_{t\to j}^{(i)} = (q_t^{(i)})^\top k_j^{(i)}$, with $q_t^{(i)} = W_q^{(i)} x_t^{(i)} + b_Q^{(i)}$ and $k_j^{(i)} = W_k^{(i)} x_j^{(i)} + b_K^{(i)}$. Expanding gives
\[
\begin{aligned}
s_{t\to j}^{(i)} &= (W_q^{(i)} x_t^{(i)})^\top (W_k^{(i)} x_j^{(i)}) + (W_q^{(i)} x_t^{(i)})^\top b_K^{(i)} \\
&\quad + (b_Q^{(i)})^\top (W_k^{(i)} x_j^{(i)}) + (b_Q^{(i)})^\top b_K^{(i)}.
\end{aligned}
\]
The third term, $\Delta_j^{(i)} \triangleq (b_Q^{(i)})^\top W_k^{(i)} x_j^{(i)}$, is a token-specific, source-agnostic shift: it raises or lowers the score for \emph{all} sources $t$ toward the same target $j$. This term represents the projection of token $j$'s representation onto the direction $(b_Q^{(i)})^\top W_k^{(i)}$. We find that this bias term for the first token, $\Delta_1^{(i)}$, is conspicuously large in most deep layers, creating a strong prior to attend to position 1. We also find that the underlying reason for the large $\Delta_1^{(i)}$ is the effective positional encoding $\mathrm{EPE}_1$. We find that $\mathrm{EPE}_1$ has very large absolute values on a small set of coordinates (this is a known phenomenon called \textit{massive activations} \YRM{Cite main paper introducing this}) which are exactly those coordinates where $(b_Q^{(i)})^\top W_k^{(i)}$ has the largest magnitude in almost all layers. This co-adaptation enables $\mathrm{EPE}_1$ to dramatically amplify $\Delta_1^{(i)}$, yielding an attention sink at the first position. \Backlog{Can we add a diagram to illustrate this? This sounds time consuming but really really helpful since this passage turned out to be somewhat dense. Let's wait to see how much space we have before doing this}

\subsection{Empirical Validation}
We validate our proposed mechanism through three complementary analyses on GPT-2, followed by causal interventions that confirm the necessity of each component described in \cref{sec:mechanism}. In \cref{sec:delta_analysis} we show that $\Delta_1^{(i)}$ is conspicuously large relative to other positions across multiple layers. Having established this, we investigate its underlying cause and show in \cref{sec:epe_alignment} that $W_k^{(i)}\mathrm{EPE}_1$ exhibits strong alignment with vector $b_Q^{(i)}$ in deep layers. In \cref{sec:wk_structure} we establish that $\mathrm{EPE}_1$ exhibits massive activations precisely at coordinates where the bias projection $b_Q^{(i)} W_k^{(i)}$ has high magnitude. Finally, in \cref{sec:interventions} we use causal interventions to verify that disrupting any component abolishes the sink while transplanting components transfers it to new positions.

\subsubsection{Bias Term Magnitude Analysis}
\label{sec:delta_analysis}
First, We verify that $\Delta_j^{(i)} = (b_Q^{(i)})^\top W_k^{(i)} x_j^{(i)}$ is anomalously large for position 1. Histograms of $\Delta_j^{(i)}$ across positions $j$ show $\Delta_1^{(i)}$ as a consistent distinct outlier. \Cref{fig:obs2_layer10} shows this for layer 10, where $\Delta_1^{(i)}$ substantially exceeds other positions (results across all layers are in Appendix \ref{app:bias_term}).

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/obs1_layer10.png}
  \caption{Distribution of bias terms $\Delta_j^{(10)}$ across positions. The first-position term $\Delta_1^{(10)}$ (red) centers at $\approx 100$, while all other positions (blue) center at $\approx -140$, demonstrating a learned preference for the first token.}
  \label{fig:obs1_layer10}
\end{figure}

\subsubsection{EPE-Bias Projection Alignment}
\label{sec:epe_alignment}
Having established the magnitude of $\Delta_1^{(i)}$, we investigate its underlying cause. Since $x_1^{(i)}$ contains both token and positional information, it remains to disentangle which of the two is responsible for the large $\Delta_1^{(i)}$. To that end, we examine alignment between $W_k^{(i)}\mathrm{EPE}$ and query bias $b_Q^{(i)}$. \Cref{fig:obs1_layer10} shows $W_k^{(10)}\mathrm{EPE}_1$ strongly aligns with $b_Q^{(10)}$, while other positions cluster near zero (full results for all layers are in Appendix \ref{app:epe_bias}).\Backlog{If we can also do that for $x_1^{(i)}-\mathrm{EPE}_1$ and show that this isn't aligned that would be great for this paragraph (we can put it in the appendix and just write it casually. This is not top priority at all.)}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/obs2_layer10.png}
  \caption{Cosine similarity between query bias $b_Q^{(10)}$ and $W_k^{(10)}\mathrm{EPE}$. $W_k^{(10)}\mathrm{EPE}_1$ (red) shows strong positive alignment ($\approx 0.7$), while other positions (blue) cluster near -0.2.}
  \label{fig:obs2_layer10}
\end{figure}

\subsubsection{Coordinate-Level Structural Analysis}
\label{sec:wk_structure}
Massive coordinates of $\mathrm{EPE}_1$ should coincide with coordinates favored by the bias projection. Let $\gamma^{(i)}=(W_k^{(i)})^\top b_Q^{(i)}\in\mathbb{R}^d$; its entry $\gamma^{(i)}[d]$ measures coordinate $d$'s contribution to source-agnostic shift $\Delta_j^{(i)}$. We identify coordinates with conspicuously large absolute values in 
$\mathrm{EPE}_1$ (see Appendix~\ref{app:massive_activations_in_ppe} for 
details). For each such coordinate $d$, we compare $|\gamma^{(i)}[d]|$ against other columns. \Cref{obs3_table} shows massive coordinates ($d{=}138,447$) substantially exceed baseline, confirming $\mathrm{EPE}_1$ is large exactly where bias projection is large (full results for all layers are in Appendix \ref{app:coor_align}).

\begin{table}[t]
  \centering
  \begin{tabular}{llll}
    \hline
    \textbf{Layer} & \textbf{Baseline (rand)} & \textbf{$d{=}138$} & \textbf{$d{=}447$}\\
    \hline
    layer 7   &   1.12$\pm$2.701    &    12.453   &    18.17         \\
    layer 9   &   1.23$\pm$3.225    &    17.846   &    26.014        \\
    layer 11  &   1.403$\pm$4.002   &    27.547   &    27.691        \\
    \hline
  \end{tabular}
  \caption{$\gamma^{(i)}=(W_k^{(i)})^\top b_Q^{(i)}$ at coordinates where $\mathrm{EPE}_1$ has massive activations (dims 138, 447) versus the baseline mean $\pm$ two standard deviations across all coordinates. The massive-$\mathrm{EPE}_1$ coordinates consistently exceed the baseline by wide margins, demonstrating that $\mathrm{EPE}_1$ is irregularly large precisely where the bias projection has strong influence.}
  \label{obs3_table}
\end{table}


\begin{figure*}[t!]
  \centering
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/obs4_no_intervention.png}
    \caption{No intervention}
    \label{fig:no_intervention}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/obs4_intervention1.png}
    \caption{ nullify $b_Q$}
    \label{fig:intervention1}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/obs4_intervention2.png}
    \caption{remove first EPE}
    \label{fig:intervention2}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/obs4_intervention3.png}
    \caption{move first EPE}
    \label{fig:intervention3}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/obs4_intervention4.png}
    \caption{nullify BOS}
    \label{fig:intervention4}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/obs4_intervention5.png}
    \caption{nullify massive activation columns of Wk}
    \label{fig:intervention5}
  \end{subfigure}
    \begin{subfigure}[t]{0.22\textwidth}
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/obs4_intervention5_2.png}
    \caption{nullify random columns of Wk}
    \label{fig:intervention5_2}
  \end{subfigure}

  \caption{Comparison of attention maps under different interventions. (a) no intervention; (b) intervention 1: nullify $b_Q$; (c) intervention 2: remove the learned EPE at position 1 and add a different EPE (the second); (d) intervention 3: transplant the learned EPE to another position (the second). (e) intervention 4: nullify BOS token embedding. intervention 5: (f) nullify massive activation columns of Wk. (g) nullify random columns of Wk.}
  \label{fig:interventions_comparison}
\end{figure*}


\subsubsection{Causal Interventions}
\label{sec:interventions}
To establish causality beyond correlation, we perform targeted 
interventions on each mechanism component during forward passes to test necessity (removing a component) and sufficiency (transplanting it) of each component. Full intervention results across all layers are provided in Appendix \ref{app:interventions}.



\begin{itemize}[leftmargin=*]
  \item \textbf{Intervention 1 — Nullify $b_Q$ (query bias is necessary).} Set $b_Q$ to zero; the sink substantially diminishes (\cref{fig:intervention1}), showing that $b_Q$ is necessary for the large first-token contribution.
  \item \textbf{Intervention 2 — Replace $\mathrm{EPE}_1$ (specificity of the positional signal).} Swap $\mathrm{EPE}_1$ with another position’s EPE; the first-position sink disappears (\cref{fig:intervention2}), indicating that $\mathrm{EPE}_1$ is critical to induce a sink.
  \item \textbf{Intervention 3 — Moving $\mathrm{EPE}_1$ induces a sink at the new token (sufficiency).} We transplant $\mathrm{EPE}_1$ from position 1 to position 2 (and give position 1 a different EPE). A strong sink forms at position 2 (\cref{fig:intervention3}), demonstrating that $\mathrm{EPE}_1$ is sufficient to elicit a sink at the new location.
  \item \textbf{Intervention 4 — BOS token does not drive the sink.} We zero the BOS token embedding before adding positional signals. The sink persists (\cref{fig:intervention4}), ruling out the embedding of the BOS token as a main driver of the sink.
  \item \textbf{Intervention 5 — Zero $W_k$ at bias-projection coordinates (structural pathway is necessary).} Zero $W_k$ columns at massive-$\mathrm{EPE}_1$ coordinates compared to zeroing $W_k$ columns at random coordinates; only the prior case substantially reduces the sink (\cref{fig:intervention5}, \cref{fig:intervention5_2}), confirming that these specific coordinates are core drivers for translating $\mathrm{EPE}_1$ into the attention bias.
\end{itemize}

\section{Conclusions}
Attention sinks are a robust emergent behavior that appears across a wide range of Transformer architectures and modalities \YRM{cite all relevant papers from RW}, but the mechanisms behind them differ across architectures. In the GPT-2–style sub-architecture we studied, we identify a concrete implementation pathway: an interaction between (i) a learned query bias, (ii) the first-layer transformation of positional information, and (iii) structure in the key projection. Crucially, this circuit cannot account for sinks in architectures that lack these components—for example, models without learned query biases \YRM{cite} or models using alternative positional schemes such as RoPE or ALiBi \YRM{cite both}, all of which have been shown to exhibit attention sinks \YRM{cite all relevant papers from RW}. This implies that while attention sinks are robust as a phenomenon, they are not governed by a single universal mechanism.

\paragraph{Implications}
The lack of a single universal mechanism reveals attention sinks as an optimization-friendly attractor: when multiple representational pathways exist, training reliably discovers circuits that implement the sink behavior. This has important implications for both understanding and controlling these phenomena. First, it suggests that attention sinks may serve a fundamental computational role that emerges regardless of specific architectural choices. Second, it indicates that effective mitigation strategies must be mechanism-aware rather than one-size-fits-all. Naive interventions targeting individual components (e.g., shrinking query biases) will likely fail, as optimization can compensate through alternative pathways. Instead, successful approaches must either address the underlying computational pressures that drive sink formation, or develop architecture-specific interventions tailored to each mechanism. 



\section{Limitations}

\subsection{Scope across architectures and scales}
Our analyses focus on a GPT-2–style model with learned query biases and absolute positional encodings. The broader Transformer ecosystem includes architectures that omit such biases or use alternative positional schemes (e.g., RoPE, ALiBi). We do not establish whether the same circuit forms in those settings, nor whether the EPE–$W_k$–$b_Q$ interaction generalizes unchanged. In addition, GPT-2 is small by contemporary standards; with scale, the mechanism could strengthen, fragment into multiple pathways, or be replaced by different circuits.

\subsection{Learning dynamics}
We provide a post-hoc, static analysis of a trained checkpoint. We do not track when the circuit emerges during pre-training, which gradients give rise to it, or whether intermediate snapshots exhibit qualitatively different pathways. Train-time causality—e.g., whether specific regularizers prevent the circuit from forming—remains outside our scope.

\subsection{Mechanism vs. function}
Our contribution is mechanistic: we explain \emph{how} an attention sink can be implemented in the studied architecture. We do not claim a definitive \emph{functional} rationale for \emph{why} such a sink is beneficial or harmful across tasks. Establishing the downstream utility or cost of the sink, and the conditions under which it is selected by optimization, is left for future work.


\bibliography{custom}

\appendix

\section{Further Experiments}

\subsection{Effective positional encoding demonstration} \label{app:epe_exp}
In this section, we illustrate that $\mathrm{EPE}_i$ roughly captures the net positional signal that is added to the input after the first layer’s transformation when adding the positional encoding $p_i$ to the input $x_i^{(0)}$. To that end, we define an approximation of result of processing the input by first MLP: $R_i \coloneq x_i^{(0)} + MLP^{(1)}(x_i^{(0)})$. We then define the input without positional information $e_i \coloneq x_i^{(0)}-p_i$, and the result of processing the input without the positional information: $O_i=e_i + MLP^{(1)}(e_i)$.  We then compare \(\mathrm{EPE}_i\) to the difference \(R_i - O_i\), computing the cosine similarity for each token \(i\). This directly measures whether the incremental contribution caused by adding \(p_i\) aligns in direction with \(\mathrm{EPE}_i\). The results range between $0.776$ at the lowest and $0.996$ at the highest. (Figure \ref{fig:epe_exp}) These similarities are very high, indicating that $EPE_i$ represents the effective contribution of positional information after being processed through the network’s initial transformations.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/epe_exp.png}
  \caption{Coordinate values of $\mathrm{EPE}_i$ for the first token (replicating the distribution described in \cref{sec:wk_structure}). Most coordinates are near zero; a small set exhibits extremely large magnitudes (``massive activations'').}
  \label{fig:epe_exp}
\end{figure}

\subsection{Bias Term Magnitude Across All Layers}\label{app:bias_term}

This section reproduces the bias-term magnitude analysis from \cref{sec:delta_analysis} across all layers: we plot the distribution of $\Delta_j^{(i)}=(b_Q^{(i)})^\top W_k^{(i)} x_j^{(i)}$ across positions for each layer (cf. \cref{fig:obs1_layer10}). In most layers, the first-position term $\Delta_1^{(i)}$ is a conspicuous outlier, indicating a strong prior to attend to position 1 (see \Cref{fig:appendix_obs1_all_layers}).

\begin{figure*}[t]
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer1.png}
    \caption{layer 1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer2.png}
    \caption{layer 2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer3.png}
    \caption{layer 3}
  \end{subfigure}\hfill
    \vspace{2mm}

  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer4.png}
    \caption{layer 4}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer5.png}
    \caption{layer 5}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer6.png}
    \caption{layer 6}
  \end{subfigure}\hfill
    \vspace{2mm}

    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer7.png}
    \caption{layer 7}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer8.png}
    \caption{layer 8}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer9.png}
    \caption{layer 9}
  \end{subfigure}\hfill
    \vspace{2mm}
    
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer10.png}
    \caption{layer 10}
  \end{subfigure}\hfill
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer11.png}
    \caption{layer 11}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs1_appendix/obs1_layer12.png}
    \caption{layer 12}
  \end{subfigure}\hfill
  \vspace{2mm}
  \caption{Bias-term distributions $\Delta_j$ across positions $j$ for each layer $i$ (replicating \cref{fig:obs1_layer10}). Red denotes the first-position term $\Delta_1^{(i)}$; blue denotes all other positions. In most layers, the red distribution is shifted far to the right, evidencing an anomalously large first-position bias.}
  \label{fig:appendix_obs1_all_layers}
\end{figure*}

\subsection{EPE-Bias Alignment Across All Layers}\label{app:epe_bias}

This section repeats the alignment analysis from \cref{sec:epe_alignment}: for each layer $i$ and position $j$ we compute $\cos\big(b_Q^{(i)},\, W_k^{(i)}\mathrm{EPE}_j\big)$, highlighting position 1. In most layers, position 1 shows strong positive alignment while other positions do not (see \Cref{fig:appendix_obs2_all_layers}).

\begin{figure*}[t]
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer1.png}
    \caption{layer 1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer2.png}
    \caption{layer 2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer3.png}
    \caption{layer 3}
  \end{subfigure}\hfill
    \vspace{2mm}

  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer4.png}
    \caption{layer 4}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer5.png}
    \caption{layer 5}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer6.png}
    \caption{layer 6}
  \end{subfigure}\hfill
    \vspace{2mm}

    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer7.png}
    \caption{layer 7}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer8.png}
    \caption{layer 8}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer9.png}
    \caption{layer 9}
  \end{subfigure}\hfill
    \vspace{2mm}
    
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer10.png}
    \caption{layer 10}
  \end{subfigure}\hfill
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer11.png}
    \caption{layer 11}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/obs2_appendix/obs2_layer12.png}
    \caption{layer 12}
  \end{subfigure}\hfill
    \vspace{2mm}

  \caption{Cosine similarity between the query bias and EPE-projected keys across layers and positions (replicating \cref{fig:obs2_layer10}). For each layer $i$ and position $j$, we plot $\cos\big(b_Q^{(i)},\, W_k^{(i)}\mathrm{EPE}_j\big)$. Red marks position $j{=}1$; blue marks all other positions. Position 1 shows strong positive alignment while other positions do not, indicating that $W_k^{(i)}\mathrm{EPE}_1$ is specifically aligned with $b_Q^{(i)}$.}
  \label{fig:appendix_obs2_all_layers}
\end{figure*}

\subsection{Identifying Massive Activations in First-Position EPE}\label{app:massive_activations_in_ppe}

This section explains how we identify coordinates with unusually large absolute values in $\mathrm{EPE}_1$. We select these coordinates by visual inspection of the $\mathrm{EPE}_1$ coordinate distribution, choosing dimensions whose magnitudes are conspicuously larger than the rest (see \Cref{fig:appendix_massive_activations}). Each such selected dimension exhibits the coordinate-level phenomenon described in \cref{sec:wk_structure} (i.e., large $|\gamma^{(i)}[d]|$ and a strong contribution to the source-agnostic shift).

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/massive_activations_in_ppe.png}
  \caption{Coordinate values of $\mathrm{EPE}_1$ for the first token (replicating the distribution described in \cref{sec:wk_structure}). Most coordinates are near zero; a small set exhibits extremely large magnitudes (``massive activations'').}
  \label{fig:appendix_massive_activations}
\end{figure}

\subsection{Coordinate-Level Alignment Across All Layers} \label{app:coor_align}

This section tabulates $\gamma^{(i)}=(W_k^{(i)})^\top b_Q^{(i)}$ at coordinates where $|\mathrm{EPE}_1|$ is conspicuously large, mirroring the coordinate-level analysis in \cref{sec:wk_structure}. Values are compared against the baseline mean $\pm$ two standard deviations across all coordinates (see \Cref{tab:appendix_coor_align}).

\begin{table}[t]
  \centering
  \begin{tabular}{llll}
    \hline
    \textbf{Layer} & \textbf{Baseline} & \textbf{$d{=}138$} & \textbf{$d{=}447$}\\
    \hline
    layer 1   &   4.47$\pm$22.226 &  12.116   &    11.064        \\
    layer 2   &   2.8$\pm$6.62    &    8.065   &    24.468         \\
    layer 3   &   1.717$\pm$5.826 &    10.178  &    23.047        \\
    layer 4   &   1.657$\pm$5.02  &   18.199   &     17.149         \\
    layer 5   &   1.561$\pm$4.618 &    3.072   &    23.854          \\
    layer 6   &   0.86$\pm$1.59   &    5.644   &    6.142          \\
    layer 8   &   1.404$\pm$3.546 &    19.806  &    28.01          \\
    layer 10  &   1.313$\pm$3.618 &    23.131  &    28.42        \\
    layer 12   &   1.145$\pm$2.65 &    4.5  &    13.59         \\

    \hline
  \end{tabular}
  \caption{$\gamma^{(i)}=(W_k^{(i)})^\top b_Q^{(i)}$ at coordinates where $\mathrm{EPE}_1$ has massive activations (dims 138, 447) versus the baseline mean $\pm$ two standard deviations across all coordinates. Massive-$\mathrm{EPE}_1$ coordinates consistently exceed the baseline, indicating that $\mathrm{EPE}_1$ is irregularly large precisely where the bias projection is large.}
  \label{tab:appendix_coor_align}
\end{table}

\subsection{Intervention Results Across All Layers}\label{app:interventions}

This section reproduces the intervention analyses from \cref{sec:interventions} across all layers, including the baseline and five targeted interventions. Each subsection mirrors the corresponding main-text figure and shows the layer-wise attention maps.

\subsubsection{Baseline: No Intervention}\label{app:no_intervention}

We show attention maps with no intervention (cf. \cref{fig:no_intervention}), demonstrating the prevalence of the first-position sink across layers (see \Cref{fig:app_no_intervention_all_layers}).
\begin{figure*}[t]
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_1.png}
    \caption{layer 1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_2.png}
    \caption{layer 2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_3.png}
    \caption{layer 3}
  \end{subfigure}\hfill

  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_4.png}
    \caption{layer 4}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_5.png}
    \caption{layer 5}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_6.png}
    \caption{layer 6}
  \end{subfigure}\hfill

    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_7.png}
    \caption{layer 7}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_8.png}
    \caption{layer 8}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_9.png}
    \caption{layer 9}
  \end{subfigure}\hfill
    
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_10.png}
    \caption{layer 10}
  \end{subfigure}\hfill
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_11.png}
    \caption{layer 11}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/no_intervention/layer_12.png}
    \caption{layer 12}
  \end{subfigure}\hfill

  \caption{Attention maps for all layers with no intervention (replicating \cref{fig:no_intervention}). A prominent first-position sink is visible in most layers.}
  \label{fig:app_no_intervention_all_layers}
\end{figure*}


\subsubsection{Intervention 1: Nullifying Query Bias}\label{app:intervention1}

We zero $b_Q$ (cf. \cref{fig:intervention1}), which substantially diminishes the sink across layers.
\begin{figure*}[t]
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_1.png}
    \caption{layer 1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_2.png}
    \caption{layer 2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_3.png}
    \caption{layer 3}
  \end{subfigure}\hfill

  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_4.png}
    \caption{layer 4}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_5.png}
    \caption{layer 5}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_6.png}
    \caption{layer 6}
  \end{subfigure}\hfill

    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_7.png}
    \caption{layer 7}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_8.png}
    \caption{layer 8}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_9.png}
    \caption{layer 9}
  \end{subfigure}\hfill
    
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_10.png}
    \caption{layer 10}
  \end{subfigure}\hfill
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_11.png}
    \caption{layer 11}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention1/layer_12.png}
    \caption{layer 12}
  \end{subfigure}\hfill

  \caption{Attention maps for all layers with $b_Q$ set to zero (replicating \cref{fig:intervention1}). The sink is substantially reduced across layers.}
\end{figure*}


\subsubsection{Intervention 2: Replacing First Position EPE}\label{app:intervention2}

We swap $\mathrm{EPE}_1$ with another position’s EPE (cf. \cref{fig:intervention2}), which removes the first-position sink.
\begin{figure*}[t]
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_1.png}
    \caption{layer 1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_2.png}
    \caption{layer 2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_3.png}
    \caption{layer 3}
  \end{subfigure}\hfill

  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_4.png}
    \caption{layer 4}
    \label{fig:intervention2_layer4}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_5.png}
    \caption{layer 5}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_6.png}
    \caption{layer 6}
  \end{subfigure}\hfill

    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_7.png}
    \caption{layer 7}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_8.png}
    \caption{layer 8}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_9.png}
    \caption{layer 9}
  \end{subfigure}\hfill
    
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_10.png}
    \caption{layer 10}
  \end{subfigure}\hfill
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_11.png}
    \caption{layer 11}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention2/layer_12.png}
    \caption{layer 12}
  \end{subfigure}\hfill

  \caption{Attention maps for all layers after swapping $\mathrm{EPE}_1$ with another position’s EPE (replicating \cref{fig:intervention2}). The first-position sink disappears.}
\end{figure*}

\subsubsection{Intervention 3: Transplanting EPE to New Position}\label{app:intervention3}

We transplant $\mathrm{EPE}_1$ from position 1 to 2 (cf. \cref{fig:intervention3}), which induces a sink at position 2.
\begin{figure*}[t]
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_1.png}
    \caption{layer 1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_2.png}
    \caption{layer 2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_3.png}
    \caption{layer 3}
  \end{subfigure}\hfill

  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_4.png}
    \caption{layer 4}
    \label{fig:intervention3_layer4}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_5.png}
    \caption{layer 5}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_6.png}
    \caption{layer 6}
  \end{subfigure}\hfill

    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_7.png}
    \caption{layer 7}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_8.png}
    \caption{layer 8}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_9.png}
    \caption{layer 9}
  \end{subfigure}\hfill
    
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_10.png}
    \caption{layer 10}
  \end{subfigure}\hfill
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_11.png}
    \caption{layer 11}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention3/layer_12.png}
    \caption{layer 12}
  \end{subfigure}\hfill

  \caption{Attention maps for all layers after moving $\mathrm{EPE}_1$ from position 1 to 2 (replicating \cref{fig:intervention3}). A strong sink forms at position 2.}
\end{figure*}

\subsubsection{Intervention 4: Nullifying BOS Token}\label{app:intervention4}

We zero the BOS token embedding prior to adding positional signals (cf. \cref{fig:intervention4}); the sink persists.
\begin{figure*}[t]
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_1.png}
    \caption{layer 1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_2.png}
    \caption{layer 2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_3.png}
    \caption{layer 3}
  \end{subfigure}\hfill

  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_4.png}
    \caption{layer 4}
    \label{fig:intervention4_layer4}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_5.png}
    \caption{layer 5}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_6.png}
    \caption{layer 6}
  \end{subfigure}\hfill

    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_7.png}
    \caption{layer 7}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_8.png}
    \caption{layer 8}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_9.png}
    \caption{layer 9}
  \end{subfigure}\hfill
    
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_10.png}
    \caption{layer 10}
  \end{subfigure}\hfill
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_11.png}
    \caption{layer 11}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention4/layer_12.png}
    \caption{layer 12}
  \end{subfigure}\hfill

  \caption{Attention maps for all layers after zeroing the BOS token embedding (replicating \cref{fig:intervention4}). The sink remains.}
\end{figure*}

\subsubsection{Intervention 5: Nullifying Massive Activation Coordinates}\label{app:intervention5}

We zero $W_k$ columns at massive-$\mathrm{EPE}_1$ coordinates (cf. \cref{fig:intervention5}), which reduces the sink far more than zeroing random columns.
\begin{figure*}[t]
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_1.png}
    \caption{layer 1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_2.png}
    \caption{layer 2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_3.png}
    \caption{layer 3}
  \end{subfigure}\hfill

  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_4.png}
    \caption{layer 4}
    \label{fig:intervention5_layer4}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_5.png}
    \caption{layer 5}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_6.png}
    \caption{layer 6}
  \end{subfigure}\hfill

    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_7.png}
    \caption{layer 7}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_8.png}
    \caption{layer 8}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_9.png}
    \caption{layer 9}
  \end{subfigure}\hfill
    
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_10.png}
    \caption{layer 10}
  \end{subfigure}\hfill
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_11.png}
    \caption{layer 11}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5/layer_12.png}
    \caption{layer 12}
  \end{subfigure}\hfill

  \caption{Attention maps for all layers after zeroing $W_k$ at massive-$\mathrm{EPE}_1$ coordinates (replicating \cref{fig:intervention5}). The sink is markedly reduced compared to random-coordinate zeroing.}
\end{figure*}

\subsubsection{Intervention 5 Control: Nullifying Random Coordinates}\label{app:intervention5_2}

As a control, we zero an equal number of random $W_k$ columns (cf. \cref{fig:intervention5_2}); the sink largely remains.
\begin{figure*}[t]
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_1.png}
    \caption{layer 1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_2.png}
    \caption{layer 2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_3.png}
    \caption{layer 3}
  \end{subfigure}\hfill

  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_4.png}
    \caption{layer 4}
    \label{fig:intervention5_2_layer4}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_5.png}
    \caption{layer 5}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_6.png}
    \caption{layer 6}
  \end{subfigure}\hfill

    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_7.png}
    \caption{layer 7}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_8.png}
    \caption{layer 8}
  \end{subfigure}\hfill
      \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_9.png}
    \caption{layer 9}
  \end{subfigure}\hfill
    
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_10.png}
    \caption{layer 10}
  \end{subfigure}\hfill
    \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_11.png}
    \caption{layer 11}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    \includegraphics[width=1.4\columnwidth]{figures/intervention5_2/layer_12.png}
    \caption{layer 12}
  \end{subfigure}\hfill

  \caption{Attention maps for all layers after zeroing random $W_k$ coordinates (replicating \cref{fig:intervention5_2}). The sink remains.}
\end{figure*}


\end{document}
