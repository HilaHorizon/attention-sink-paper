% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS (NIPS) 2017},
  year={2017},
  url={https://arxiv.org/abs/1706.03762}
}
@article{gu2025when,
  title={When Attention Sink Emerges in Language Models: An Empirical View},
  author={Gu, Xiangming and Pang, Tianyu and Du, Chao and Liu, Qian and Zhang, Fengzhuo and Du, Cunxiao and Wang, Ye and Lin, Min},
  journal={arXiv preprint arXiv:2410.10781},
  year={2025}
}
@article{sun2024massive,
  title={Massive Activations in Large Language Models},
  author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
  journal={arXiv preprint arXiv:2402.17762},
  year={2024}
}

@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunbo},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@misc{touvron2023llama2,
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author       = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang and Kuan Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year         = {2023},
  eprint       = {2307.09288},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  note         = {arXiv:2307.09288v2},
  url          = {https://arxiv.org/abs/2307.09288}
}


@inproceedings{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={3533--3547},
  year={2021}
}
@misc{bigscience2023bloom,
  title        = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author       = {{BigScience Workshop}},
  year         = {2023},
  eprint       = {2211.05100},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  note         = {arXiv:2211.05100v4},
  url          = {https://arxiv.org/abs/2211.05100}
}


@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@misc{lindnielsen2024spectral,
      title={Spectral Filters, Dark Signals, and Attention Sinks}, 
      author={Nicola Cancedda},
      year={2024},
      eprint={2402.09221},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{lin2024duquant,
  title     = {DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs},
  author    = {Haokun Lin and Haobo Xu and Yichen Wu and Jingzhi Cui and Yingtao Zhang and Linzhan Mou and Linqi Song and Zhenan Sun and Ying Wei},
  booktitle = {Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS 2024)},
  year      = {2024},
  note      = {arXiv preprint arXiv:2406.01721v3},
  eprint    = {2406.01721},
  archivePrefix = {arXiv},
  url       = {https://arxiv.org/abs/2406.01721}
}
@article{Yu2024Unveiling,
title={Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration},
author={Zhongzhi Yu and Zheng Wang and Yonggan Fu and Huihong Shi and Khalid Shaikh and Yingyan (Celine) Lin },
journal={ArXiv},
year={2024},
volume={abs/2406.15765},
doi={10.48550/arXiv.2406.15765}}

@article{Kang2025See,
title={See What You Are Told: Visual Attention Sink in Large Multimodal Models},
author={Seil Kang and Jinyeong Kim and Junhyeok Kim and Seong Jae Hwang},
journal={ArXiv},
year={2025},
volume={abs/2503.03321},
doi={10.48550/arXiv.2503.03321}}

@article{Feng2025EDIT:,
title={EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture},
author={Wenfeng Feng and Guoying Sun},
journal={ArXiv},
year={2025},
volume={abs/2504.06738},
doi={10.48550/arXiv.2504.06738}}

@article{Barbero2025Why,
title={Why do LLMs attend to the first token?},
author={Federico Barbero and 'Alvaro Arroyo and Xiangming Gu and Christos Perivolaropoulos and Michael M. Bronstein and Petar Velivckovi 'c and Razvan Pascanu},
journal={ArXiv},
year={2025},
volume={abs/2504.02732},
doi={10.48550/arXiv.2504.02732}}

@article{Zuhri2025SoftpickNA,
  title={Softpick: No Attention Sink, No Massive Activations with Rectified Softmax},
  author={Zayd Muhammad Kawakibi Zuhri and Erland Hilman Fuadi and Alham Fikri Aji},
  journal={ArXiv},
  year={2025},
  volume={abs/2504.20966},
  url={https://api.semanticscholar.org/CorpusID:278171209}
}

@inproceedings{endy-etal-2025-mamba,
    title = "Mamba Knockout for Unraveling Factual Information Flow",
    author = "Endy, Nir  and
      Grosbard, Idan Daniel  and
      Ran-Milo, Yuval  and
      Slutzky, Yonatan  and
      Tshuva, Itay  and
      Giryes, Raja",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1143/",
    doi = "10.18653/v1/2025.acl-long.1143",
    pages = "23457--23477",
    ISBN = "979-8-89176-251-0",
    abstract = "This paper investigates the flow of factual information in Mamba State-Space Model (SSM)-based language models. We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms. Exploiting this relationship, we adapt attentional interpretability techniques originally developed for Transformers{---}specifically, the Attention Knockout methodology{---}to both Mamba-1 and Mamba-2. Using them we trace how information is transmitted and localized across tokens and layers, revealing patterns of subject-token information emergence and layer-wise dynamics. Notably, some phenomena vary between mamba models and Transformer based models, while others appear universally across all models inspected{---}hinting that these may be inherent to LLMs in general. By further leveraging Mamba{'}s structured factorization, we disentangle how distinct ``features'' either enable token-to-token information exchange or enrich individual tokens, thus offering a unified lens to understand Mamba internal operations."
}

@misc{wang2025mirage,
      title={Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink}, 
      author={Yining Wang and Mi Zhang and Junjie Sun and Chenyue Wang and Min Yang and Hui Xue and Jialing Tao and Ranjie Duan and Jiexi Liu},
      year={2025},
      eprint={2501.15269},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Guo2024ActiveDormantAH,
  title={Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs},
  author={Tianyu Guo and Druv Pai and Yu Bai and Jiantao Jiao and Michael I. Jordan and Song Mei},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.13835},
  url={https://api.semanticscholar.org/CorpusID:273403764}
}

@article{Irie2019LanguageMW,
  title={Language Modeling with Deep Transformers},
  author={Kazuki Irie and Albert Zeyer and Ralf Schl{\"u}ter and Hermann Ney},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.04226},
  url={https://api.semanticscholar.org/CorpusID:150373667}
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}

@unknown{Yang2024Qwen2,
author = {Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Dong, Guanting and Wei, Haoran and Lin, Huan and Tang, Jialong and Wang, Jialin and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Ma, Jianxin and Fan, Zhihao},
year = {2024},
month = {07},
pages = {},
title = {Qwen2 Technical Report},
doi = {10.48550/arXiv.2407.10671}
}

@article{Zeng2024ChatGLMAF,
  title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools},
  author={Team Glm Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiaoyu Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yi An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhenyi Yang and Zhengxiao Du and Zhen-Ping Hou and Zihan Wang},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.12793},
  url={https://api.semanticscholar.org/CorpusID:270562306}
}

@article{Zhang2022OPTOP,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona T. Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.01068},
  url={https://api.semanticscholar.org/CorpusID:248496292}
}

@inproceedings{Dehghani2023ScalingVT,
  title={Scaling Vision Transformers to 22 Billion Parameters},
  author={Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim M. Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F. Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Collier and Alexey A. Gritsenko and Vighnesh Birodkar and Cristina Nader Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Paveti'c and Dustin Tran and Thomas Kipf and Mario Luvci'c and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby},
  booktitle={International Conference on Machine Learning},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:256808367}
}

@article{Chowdhery2022PaLMSL,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark D{\'i}az and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.02311},
  url={https://api.semanticscholar.org/CorpusID:247951931}
}