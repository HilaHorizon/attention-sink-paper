% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS (NIPS) 2017},
  year={2017},
  url={https://arxiv.org/abs/1706.03762}
}
@article{gu2025when,
  title={When Attention Sink Emerges in Language Models: An Empirical View},
  author={Gu, Xiangming and Pang, Tianyu and Du, Chao and Liu, Qian and Zhang, Fengzhuo and Du, Cunxiao and Wang, Ye and Lin, Min},
  journal={arXiv preprint arXiv:2410.10781},
  year={2025}
}
@article{sun2024massive,
  title={Massive Activations in Large Language Models},
  author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
  journal={arXiv preprint arXiv:2402.17762},
  year={2024}
}

@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunbo},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@misc{touvron2023llama2,
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author       = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang and Kuan Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year         = {2023},
  eprint       = {2307.09288},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  note         = {arXiv:2307.09288v2},
  url          = {https://arxiv.org/abs/2307.09288}
}


@inproceedings{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={3533--3547},
  year={2021}
}
@misc{bigscience2023bloom,
  title        = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author       = {{BigScience Workshop}},
  year         = {2023},
  eprint       = {2211.05100},
  archivePrefix= {arXiv},
  primaryClass = {cs.CL},
  note         = {arXiv:2211.05100v4},
  url          = {https://arxiv.org/abs/2211.05100}
}


@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@misc{lindnielsen2024spectral,
      title={Spectral Filters, Dark Signals, and Attention Sinks}, 
      author={Nicola Cancedda},
      year={2024},
      eprint={2402.09221},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{lin2024duquant,
  title     = {DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs},
  author    = {Haokun Lin and Haobo Xu and Yichen Wu and Jingzhi Cui and Yingtao Zhang and Linzhan Mou and Linqi Song and Zhenan Sun and Ying Wei},
  booktitle = {Proceedings of the 38th Conference on Neural Information Processing Systems (NeurIPS 2024)},
  year      = {2024},
  note      = {arXiv preprint arXiv:2406.01721v3},
  eprint    = {2406.01721},
  archivePrefix = {arXiv},
  url       = {https://arxiv.org/abs/2406.01721}
}
@article{Yu2024Unveiling,
title={Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration},
author={Zhongzhi Yu and Zheng Wang and Yonggan Fu and Huihong Shi and Khalid Shaikh and Yingyan (Celine) Lin },
journal={ArXiv},
year={2024},
volume={abs/2406.15765},
doi={10.48550/arXiv.2406.15765}}

@article{Kang2025See,
title={See What You Are Told: Visual Attention Sink in Large Multimodal Models},
author={Seil Kang and Jinyeong Kim and Junhyeok Kim and Seong Jae Hwang},
journal={ArXiv},
year={2025},
volume={abs/2503.03321},
doi={10.48550/arXiv.2503.03321}}

@article{Feng2025EDIT:,
title={EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture},
author={Wenfeng Feng and Guoying Sun},
journal={ArXiv},
year={2025},
volume={abs/2504.06738},
doi={10.48550/arXiv.2504.06738}}

@article{Barbero2025Why,
title={Why do LLMs attend to the first token?},
author={Federico Barbero and 'Alvaro Arroyo and Xiangming Gu and Christos Perivolaropoulos and Michael M. Bronstein and Petar Velivckovi 'c and Razvan Pascanu},
journal={ArXiv},
year={2025},
volume={abs/2504.02732},
doi={10.48550/arXiv.2504.02732}}

@article{Zuhri2025SoftpickNA,
  title={Softpick: No Attention Sink, No Massive Activations with Rectified Softmax},
  author={Zayd Muhammad Kawakibi Zuhri and Erland Hilman Fuadi and Alham Fikri Aji},
  journal={ArXiv},
  year={2025},
  volume={abs/2504.20966},
  url={https://api.semanticscholar.org/CorpusID:278171209}
}

@inproceedings{endy-etal-2025-mamba,
    title = "Mamba Knockout for Unraveling Factual Information Flow",
    author = "Endy, Nir  and
      Grosbard, Idan Daniel  and
      Ran-Milo, Yuval  and
      Slutzky, Yonatan  and
      Tshuva, Itay  and
      Giryes, Raja",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1143/",
    doi = "10.18653/v1/2025.acl-long.1143",
    pages = "23457--23477",
    ISBN = "979-8-89176-251-0",
    abstract = "This paper investigates the flow of factual information in Mamba State-Space Model (SSM)-based language models. We rely on theoretical and empirical connections to Transformer-based architectures and their attention mechanisms. Exploiting this relationship, we adapt attentional interpretability techniques originally developed for Transformers{---}specifically, the Attention Knockout methodology{---}to both Mamba-1 and Mamba-2. Using them we trace how information is transmitted and localized across tokens and layers, revealing patterns of subject-token information emergence and layer-wise dynamics. Notably, some phenomena vary between mamba models and Transformer based models, while others appear universally across all models inspected{---}hinting that these may be inherent to LLMs in general. By further leveraging Mamba{'}s structured factorization, we disentangle how distinct ``features'' either enable token-to-token information exchange or enrich individual tokens, thus offering a unified lens to understand Mamba internal operations."
}